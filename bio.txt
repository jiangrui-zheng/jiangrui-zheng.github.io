Jiangrui Zheng is a Computer Science PhD student at Stevens Institute of Technology specializing in large language models for security and software engineering. His work focuses on building practical AI systems that improve vulnerability analysis, automate security workflows, and evaluate the reliability of SOTA LLMs. He has developed LLM-based agents for verifying security vulnerability reports, automated red-teaming pipelines for testing hate-speech defenses, and test-case generators for model management on HuggingFace Hub. His research also includes NER-driven automation for handling software versions in vulnerability reports and empirical studies on AI-assisted code review and patch retrieval. Jiangruiâ€™s work has been published at NAACL and IEEE BigData workshops, and he has contributed to multiple projects on patch tracing, explainable retrieval, and analyzing risks in AI-generated code. He aims to build trustworthy intelligent systems that strengthen software security at scale.